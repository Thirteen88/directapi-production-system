#!/usr/bin/env python3
"""
Smart Caching System for DirectAPI
Intelligent LRU cache with semantic similarity matching and TTL management
Reduces redundant API calls and improves response times for repeated queries
"""

import asyncio
import aiohttp
import json
import time
import hashlib
import logging
import re
from typing import Dict, Any, List, Optional, Tuple
from dataclasses import dataclass, field
from collections import OrderedDict
import difflib
import math

# Setup logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

@dataclass
class CacheEntry:
    """Cache entry with metadata"""
    content: str
    model_used: str
    tokens_used: int
    response_time: float
    timestamp: float
    ttl: float  # Time to live in seconds
    hit_count: int = 0
    last_accessed: float = field(default_factory=time.time)
    semantic_hash: str = ""
    prompt_hash: str = ""
    tags: List[str] = field(default_factory=list)

@dataclass
class CacheStats:
    """Cache statistics"""
    total_requests: int = 0
    cache_hits: int = 0
    cache_misses: int = 0
    semantic_hits: int = 0
    exact_hits: int = 0
    evictions: int = 0
    total_tokens_saved: int = 0
    total_time_saved: float = 0.0
    avg_response_time: float = 0.0

class SemanticSimilarityMatcher:
    """Advanced semantic similarity matching for cache hits"""

    def __init__(self):
        self.stop_words = {
            "the", "a", "an", "and", "or", "but", "in", "on", "at", "to", "for",
            "of", "with", "by", "is", "are", "was", "were", "be", "been", "have",
            "has", "had", "do", "does", "did", "will", "would", "could", "should",
            "can", "may", "might", "must", "i", "you", "he", "she", "it", "we", "they"
        }

    def normalize_text(self, text: str) -> str:
        """Normalize text for comparison"""
        # Convert to lowercase and remove special characters
        text = text.lower()
        text = re.sub(r'[^\w\s]', ' ', text)

        # Remove stop words
        words = text.split()
        words = [word for word in words if word not in self.stop_words]

        return ' '.join(words)

    def calculate_jaccard_similarity(self, text1: str, text2: str) -> float:
        """Calculate Jaccard similarity between two texts"""
        words1 = set(self.normalize_text(text1).split())
        words2 = set(self.normalize_text(text2).split())

        intersection = len(words1.intersection(words2))
        union = len(words1.union(words2))

        return intersection / union if union > 0 else 0.0

    def calculate_cosine_similarity(self, text1: str, text2: str) -> float:
        """Calculate cosine similarity using word frequency vectors"""
        text1 = self.normalize_text(text1)
        text2 = self.normalize_text(text2)

        # Create word frequency dictionaries
        words1 = text1.split()
        words2 = text2.split()

        all_words = set(words1 + words2)

        # Create frequency vectors
        freq1 = {word: words1.count(word) for word in all_words}
        freq2 = {word: words2.count(word) for word in all_words}

        # Calculate dot product and magnitudes
        dot_product = sum(freq1[word] * freq2.get(word, 0) for word in all_words)
        magnitude1 = math.sqrt(sum(freq1[word] ** 2 for word in all_words))
        magnitude2 = math.sqrt(sum(freq2[word] ** 2 for word in all_words))

        return dot_product / (magnitude1 * magnitude2) if magnitude1 * magnitude2 > 0 else 0.0

    def calculate_similarity(self, text1: str, text2: str) -> float:
        """Calculate overall similarity score"""
        jaccard = self.calculate_jaccard_similarity(text1, text2)
        cosine = self.calculate_cosine_similarity(text1, text2)

        # Weighted average (Jaccard is better for semantic similarity)
        return (jaccard * 0.7 + cosine * 0.3)

class SmartCache:
    """Intelligent caching system with LRU and semantic matching"""

    def __init__(self, max_size: int = 1000, default_ttl: float = 3600, semantic_threshold: float = 0.8):
        self.max_size = max_size
        self.default_ttl = default_ttl
        self.semantic_threshold = semantic_threshold

        # Use OrderedDict for LRU eviction
        self.cache = OrderedDict()
        self.semantic_matcher = SemanticSimilarityMatcher()
        self.stats = CacheStats()

        # Index for semantic matching
        self.semantic_index = {}  # semantic_hash -> prompt_hash
        self.prompt_index = {}    # prompt_hash -> CacheEntry

        logger.info(f"üß† Smart Cache initialized: max_size={max_size}, ttl={default_ttl}s, semantic_threshold={semantic_threshold}")

    def _create_prompt_hash(self, prompt: str, system_prompt: str = None) -> str:
        """Create hash for prompt"""
        content = f"{system_prompt or ''}|||{prompt}"
        return hashlib.md5(content.encode()).hexdigest()

    def _create_semantic_hash(self, prompt: str) -> str:
        """Create semantic hash for content-based matching"""
        normalized = self.semantic_matcher.normalize_text(prompt)
        return hashlib.md5(normalized.encode()).hexdigest()

    def _is_expired(self, entry: CacheEntry) -> bool:
        """Check if cache entry is expired"""
        return time.time() - entry.timestamp > entry.ttl

    def _evict_expired(self):
        """Remove expired entries"""
        expired_keys = []
        for key, entry in self.cache.items():
            if self._is_expired(entry):
                expired_keys.append(key)

        for key in expired_keys:
            del self.cache[key]
            self.stats.evictions += 1

        if expired_keys:
            logger.info(f"üßπ Evicted {len(expired_keys)} expired cache entries")

    def _evict_lru(self):
        """Evict least recently used entries"""
        while len(self.cache) >= self.max_size:
            # Remove oldest entry
            oldest_key = next(iter(self.cache))
            del self.cache[oldest_key]
            self.stats.evictions += 1

    async def get(self, prompt: str, system_prompt: str = None, model: str = None) -> Optional[CacheEntry]:
        """Get cached response"""
        self.stats.total_requests += 1

        prompt_hash = self._create_prompt_hash(prompt, system_prompt)

        # Exact match first
        if prompt_hash in self.cache:
            entry = self.cache[prompt_hash]

            # Update access info
            entry.last_accessed = time.time()
            entry.hit_count += 1

            # Move to end (LRU)
            self.cache.move_to_end(prompt_hash)

            self.stats.cache_hits += 1
            self.stats.exact_hits += 1

            logger.debug(f"üéØ Exact cache hit for prompt_hash: {prompt_hash[:8]}...")
            return entry

        # Semantic matching
        semantic_hash = self._create_semantic_hash(prompt)

        if semantic_hash in self.semantic_index:
            similar_prompts = self.semantic_index[semantic_hash]

            for similar_hash in similar_prompts:
                if similar_hash in self.cache:
                    similar_entry = self.cache[similar_hash]

                    # Check if not expired
                    if not self._is_expired(similar_entry):
                        # Calculate similarity
                        similarity = self.semantic_matcher.calculate_similarity(
                            prompt, similar_entry.prompt_hash
                        )

                        if similarity >= self.semantic_threshold:
                            # Update access info
                            similar_entry.last_accessed = time.time()
                            similar_entry.hit_count += 1
                            self.cache.move_to_end(similar_hash)

                            self.stats.cache_hits += 1
                            self.stats.semantic_hits += 1

                            logger.info(f"üß† Semantic cache hit: {similarity:.2f} similarity")
                            return similar_entry

        self.stats.cache_misses += 1
        return None

    async def put(self, prompt: str, system_prompt: str, content: str,
                  model: str, tokens_used: int, response_time: float,
                  ttl: float = None, tags: List[str] = None) -> CacheEntry:
        """Store response in cache"""

        # Clean up expired entries
        self._evict_expired()

        # Ensure space available
        self._evict_lru()

        # Create cache entry
        prompt_hash = self._create_prompt_hash(prompt, system_prompt)
        semantic_hash = self._create_semantic_hash(prompt)

        entry = CacheEntry(
            content=content,
            model_used=model,
            tokens_used=tokens_used,
            response_time=response_time,
            timestamp=time.time(),
            ttl=ttl or self.default_ttl,
            semantic_hash=semantic_hash,
            prompt_hash=prompt_hash,
            tags=tags or []
        )

        # Store in cache
        self.cache[prompt_hash] = entry
        self.cache.move_to_end(prompt_hash)

        # Update semantic index
        if semantic_hash not in self.semantic_index:
            self.semantic_index[semantic_hash] = []
        self.semantic_index[semantic_hash].append(prompt_hash)

        # Update prompt index
        self.prompt_index[prompt_hash] = entry

        logger.debug(f"üíæ Cached response: {prompt_hash[:8]}... (TTL: {entry.ttl}s)")

        return entry

    def get_stats(self) -> Dict[str, Any]:
        """Get cache statistics"""
        total_requests = self.stats.total_requests

        if total_requests > 0:
            hit_rate = (self.stats.cache_hits / total_requests) * 100
            exact_hit_rate = (self.stats.exact_hits / total_requests) * 100
            semantic_hit_rate = (self.stats.semantic_hits / total_requests) * 100
        else:
            hit_rate = 0
            exact_hit_rate = 0
            semantic_hit_rate = 0

        return {
            "cache_size": len(self.cache),
            "max_size": self.max_size,
            "total_requests": total_requests,
            "cache_hits": self.stats.cache_hits,
            "cache_misses": self.stats.cache_misses,
            "hit_rate": f"{hit_rate:.1f}%",
            "exact_hits": self.stats.exact_hits,
            "semantic_hits": self.stats.semantic_hits,
            "exact_hit_rate": f"{exact_hit_rate:.1f}%",
            "semantic_hit_rate": f"{semantic_hit_rate:.1f}%",
            "evictions": self.stats.evictions,
            "total_tokens_saved": self.stats.total_tokens_saved,
            "total_time_saved": self.stats.total_time_saved,
            "avg_response_time": self.stats.avg_response_time,
            "cache_efficiency": f"{self.stats.total_time_saved / (self.stats.total_time_saved + 1):.1f}s saved per hit"
        }

    def clear(self):
        """Clear all cache entries"""
        self.cache.clear()
        self.semantic_index.clear()
        self.prompt_index.clear()
        logger.info("üßπ Cache cleared")

class CachedDirectAPIClient:
    """DirectAPI client with intelligent caching"""

    def __init__(self, cache_size: int = 1000, cache_ttl: float = 3600, semantic_threshold: float = 0.8):
        self.cache = SmartCache(cache_size, cache_ttl, semantic_threshold)
        self.base_url = "https://chat3.eqing.tech/v1"

        # Working headers from successful testing
        self.headers = {
            'Content-Type': 'application/json',
            'Accept': 'application/json',
            'Accept-Language': 'en-US,en;q=0.9',
            'Accept-Encoding': 'gzip, deflate, br',
            'Cache-Control': 'no-cache',
            'Pragma': 'no-cache',
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Origin': 'https://chat3.eqing.tech',
            'Referer': 'https://chat3.eqing.tech/',
            'Sec-Fetch-Dest': 'empty',
            'Sec-Fetch-Mode': 'cors',
            'Sec-Fetch-Site': 'same-origin',
            'Sec-Ch-UA': '"Not_A Brand";v="8", "Chromium";v="120"',
            'Sec-Ch-UA-Mobile': '?0',
            'Sec-Ch-UA-Platform': '"Windows"'
        }

        self.stats = {
            "total_requests": 0,
            "cached_requests": 0,
            "api_requests": 0,
            "total_time": 0.0,
            "cache_time_saved": 0.0,
            "tokens_saved": 0
        }

    async def generate_response(self, prompt: str, system_prompt: str = None,
                              model: str = "gpt-4o-mini", max_tokens: int = 1500,
                              temperature: float = 0.7, force_refresh: bool = False) -> Dict[str, Any]:
        """Generate response with intelligent caching"""

        start_time = time.time()
        self.stats["total_requests"] += 1

        # Try cache first (unless force refresh)
        if not force_refresh:
            cached_entry = await self.cache.get(prompt, system_prompt, model)

            if cached_entry:
                cache_time = time.time() - start_time
                self.stats["cached_requests"] += 1
                self.stats["cache_time_saved"] += cached_entry.response_time
                self.stats["tokens_saved"] += cached_entry.tokens_used

                logger.info(f"üéØ Cache hit: {cached_entry.response_time:.2f}s saved")

                return {
                    "success": True,
                    "content": cached_entry.content,
                    "model_used": cached_entry.model_used,
                    "tokens_used": cached_entry.tokens_used,
                    "response_time": cache_time,
                    "from_cache": True,
                    "cache_hit_count": cached_entry.hit_count
                }

        # Cache miss - make API call
        logger.info(f"üåê API call: {model} for prompt")

        try:
            # Prepare request
            messages = []
            if system_prompt:
                messages.append({"role": "system", "content": system_prompt})
            messages.append({"role": "user", "content": prompt})

            data = {
                "model": model,
                "messages": messages,
                "temperature": temperature,
                "max_tokens": max_tokens
            }

            # Make API call
            timeout = aiohttp.ClientTimeout(total=60)
            async with aiohttp.ClientSession(timeout=timeout) as session:
                async with session.post(
                    f"{self.base_url}/chat/completions",
                    headers=self.headers,
                    json=data
                ) as response:

                    api_time = time.time() - start_time
                    response_text = await response.text()

                    # Handle 403 with data (expected behavior)
                    if response.status == 403:
                        logger.info("üì• Got 403 with data - extracting response (expected)")

                    try:
                        response_data = json.loads(response_text)

                        if response_data.get("choices") and response_data["choices"][0]:
                            content = response_data["choices"][0].get("message", {}).get("content", "")
                            tokens_used = response_data.get("usage", {}).get("total_tokens", 0)

                            # Cache the result
                            await self.cache.put(
                                prompt=prompt,
                                system_prompt=system_prompt,
                                content=content,
                                model=model,
                                tokens_used=tokens_used,
                                response_time=api_time
                            )

                            self.stats["api_requests"] += 1
                            self.stats["total_time"] += api_time

                            # Update cache stats
                            cache_stats = self.cache.get_stats()
                            if cache_stats["cache_hits"] > 0:
                                self.stats["cache_time_saved"] = cache_stats["total_time_saved"]
                                self.stats["tokens_saved"] = cache_stats["total_tokens_saved"]

                            logger.info(f"‚úÖ API call successful: {api_time:.2f}s, {tokens_used} tokens")

                            return {
                                "success": True,
                                "content": content,
                                "model_used": model,
                                "tokens_used": tokens_used,
                                "response_time": api_time,
                                "from_cache": False,
                                "cached": True
                            }
                        else:
                            self.stats["api_requests"] += 1
                            self.stats["total_time"] += api_time

                            return {
                                "success": False,
                                "error": "No choices in response",
                                "response_time": api_time,
                                "from_cache": False
                            }

                    except json.JSONDecodeError:
                        self.stats["api_requests"] += 1
                        self.stats["total_time"] += api_time

                        return {
                            "success": False,
                            "error": f"Invalid JSON: {response_text[:100]}",
                            "response_time": api_time,
                            "from_cache": False
                        }

        except Exception as e:
            total_time = time.time() - start_time
            self.stats["total_time"] += total_time
            logger.error(f"‚ùå API call failed: {e}")

            return {
                "success": False,
                "error": str(e),
                "response_time": total_time,
                "from_cache": False
            }

    def get_performance_stats(self) -> Dict[str, Any]:
        """Get comprehensive performance statistics"""
        cache_stats = self.cache.get_stats()

        if self.stats["total_requests"] > 0:
            cache_efficiency = (self.stats["cached_requests"] / self.stats["total_requests"]) * 100
            avg_response_time = self.stats["total_time"] / self.stats["total_requests"]
        else:
            cache_efficiency = 0
            avg_response_time = 0

        return {
            "client_stats": self.stats,
            "cache_stats": cache_stats,
            "performance_metrics": {
                "cache_efficiency": f"{cache_efficiency:.1f}%",
                "avg_response_time": f"{avg_response_time:.2f}s",
                "time_saved_per_hit": f"{self.stats['cache_time_saved'] / max(1, cache_stats['cache_hits']):.2f}s",
                "tokens_saved_per_hit": f"{self.stats['tokens_saved'] / max(1, cache_stats['cache_hits']):.0f}",
                "overall_improvement": f"{1 + (self.stats['cache_time_saved'] / max(1, self.stats['total_time'])):.1f}x faster with cache"
            }
        }

async def test_caching_system():
    """Test the caching system with various scenarios"""
    print("üß™ Testing Smart Caching System")
    print("="*60)

    client = CachedDirectAPIClient(cache_size=100, cache_ttl=300, semantic_threshold=0.7)

    test_scenarios = [
        {
            "name": "Code Generation",
            "prompt": "Write a Python function to validate email addresses",
            "system_prompt": "You are a Python expert. Write clean, efficient code."
        },
        {
            "name": "Code Generation (Similar)",
            "prompt": "Create a Python function for email address validation",
            "system_prompt": "You are a Python expert. Write clean, efficient code."
        },
        {
            "name": "Code Generation (Different)",
            "prompt": "Write a JavaScript function to validate email addresses",
            "system_prompt": "You are a JavaScript expert. Write clean, efficient code."
        },
        {
            "name": "Debugging",
            "prompt": "Debug this Python code: def test(): print('hello')",
            "system_prompt": "You are a debugging expert."
        },
        {
            "name": "Documentation",
            "prompt": "Write documentation for a REST API endpoint",
            "system_prompt": "You are a technical writer."
        }
    ]

    results = []

    print(f"\nüìù Testing {len(test_scenarios)} scenarios...")

    for scenario in test_scenarios:
        print(f"\nüîÑ {scenario['name']}")

        # First call (cache miss)
        result1 = await client.generate_response(
            prompt=scenario["prompt"],
            system_prompt=scenario["system_prompt"]
        )

        if result1["success"]:
            print(f"   First call: {result1['response_time']:.2f}s (cached: {result1.get('cached', False)})")

        # Second call (should be cache hit if similar)
        result2 = await client.generate_response(
            prompt=scenario["prompt"],
            system_prompt=scenario["system_prompt"]
        )

        if result2["success"]:
            print(f"   Second call: {result2['response_time']:.2f}s (cached: {result2.get('cached', False)})")
            if result2.get("cache_hit_count"):
                print(f"   Cache hits: {result2['cache_hit_count']}")

        results.append({
            "scenario": scenario["name"],
            "first_call": result1,
            "second_call": result2,
            "time_saved": result1["response_time"] - result2["response_time"] if result2["success"] else 0
        })

    # Show performance stats
    stats = client.get_performance_stats()
    print(f"\nüìä Caching System Performance:")
    print(f"   Cache Efficiency: {stats['performance_metrics']['cache_efficiency']}")
    print(f"   Overall Improvement: {stats['performance_metrics']['overall_improvement']}")
    print(f"   Time Saved Per Hit: {stats['performance_metrics']['time_saved_per_hit']}")
    print(f"   Cache Size: {stats['cache_stats']['cache_size']}/{stats['cache_stats']['max_size']}")
    print(f"   Hit Rate: {stats['cache_stats']['hit_rate']}")
    print(f"   Exact Hits: {stats['cache_stats']['exact_hit_rate']}")
    print(f"   Semantic Hits: {stats['cache_stats']['semantic_hit_rate']}")

    return results, stats

async def main():
    """Main caching system demo"""
    try:
        results, stats = await test_caching_system()

        print(f"\n‚úÖ Smart Caching System Test Complete!")
        print(f"   Demonstrated intelligent caching with semantic similarity matching")
        print(f"   Cache hits reduce API calls and improve response times")
        print(f"   Ready for integration with DirectAPI agents")

    except Exception as e:
        logger.error(f"‚ùå Caching test failed: {e}")
        print(f"‚ùå Test failed: {e}")

if __name__ == "__main__":
    asyncio.run(main())